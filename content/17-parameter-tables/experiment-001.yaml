learning_rate: 0.001
batch_size: 32
epochs: 100
dropout: 0.2
layers: 4
hidden_units: 256
activation: relu
accuracy: 0.923
loss: 0.187
f1_score: 0.918
