learning_rate: 0.001
batch_size: 64
epochs: 150
dropout: 0.3
layers: 8
hidden_units: 512
activation: gelu
accuracy: 0.947
loss: 0.142
f1_score: 0.944
