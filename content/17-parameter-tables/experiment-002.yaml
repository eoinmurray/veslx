learning_rate: 0.01
batch_size: 32
epochs: 100
dropout: 0.2
layers: 4
hidden_units: 256
activation: relu
accuracy: 0.891
loss: 0.243
f1_score: 0.885
